{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47b1f051-d66d-4662-9abd-aed002d40348",
   "metadata": {},
   "source": [
    "# Polyglot-ko-12.8b 모델을 KoALPACA 모델로의 학습 후 평가"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424e4efd-9f1f-4a99-8626-e3b0b9e46caf",
   "metadata": {},
   "source": [
    "## 1. 개요\n",
    "* 모델명 :[meta-llama/Llama-2-7b-hf](https://huggingface.co/meta-llama/Llama-2-7b-hf) 모델을 아래의 QLoRA로 SFT 한 모델\n",
    "* 데이터셋\n",
    "    * 한국어 Alpaca Dataset : [ko_alpaca_data.json](https://github.com/Beomi/KoAlpaca/blob/main/ko_alpaca_data.json)\n",
    "    * 네이버 지식인 베스트 데이터 : [KoAlpaca_v1.1.json](https://raw.githubusercontent.com/Beomi/KoAlpaca/main/KoAlpaca_v1.1.jsonl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3de0d054-a301-453b-afbf-60cba7f15e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from peft import (\n",
    "    prepare_model_for_kbit_training,\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    PeftModel,\n",
    ")\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    GenerationConfig,\n",
    "    TextStreamer\n",
    ")\n",
    "\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab4473e-780e-409d-893f-94bc94c48db5",
   "metadata": {},
   "source": [
    "## 2. Set Arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e35093-bae0-42d2-814d-9dd45a43c21d",
   "metadata": {},
   "source": [
    "### 2.1 base 관련 파라미터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "657ba9bd-39ce-4f4f-bbfe-0a4ee52a690e",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_DIR = '/workspace/polyglot_koalpaca_finetuning/output/expt-4epochs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a8c0fc9-b231-4ad7-b5b1-3f416c1d49fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = '/workspace/polyglot_koalpaca_finetuning'\n",
    "RANDOM_SEED = 777\n",
    "HUGGINGFACE_TOKEN = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0a8f41c-74b0-4eec-9fbe-e2dda7a4b810",
   "metadata": {},
   "outputs": [],
   "source": [
    "CACHE_DIR=os.path.join('/workspace', \".cache\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56eb96b8-b42c-4a53-a90f-b75fc5180905",
   "metadata": {},
   "source": [
    "### 2.2 Model 관련 파라미터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67468599-7e90-43cc-be99-096fb2097cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME_OR_PATH = 'beomi/polyglot-ko-12.8b-safetensors'\n",
    "TORCH_DTYPE=torch.float16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54b8ddf-4146-4d75-80e8-da26817e01b7",
   "metadata": {},
   "source": [
    "### 2.3 양자화 관련 파라미터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab47993e-c06b-4798-ad4b-96e6ee4f7cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOAD_IN_4BIT=True                                   # Enable 4-bit quantization\n",
    "BNB_4BIT_QUANT_TYPE=\"nf4\"                           # BNB 4-bit quantization type\n",
    "BNB_4BIT_COMPUTE_DTYPE=torch.bfloat16               # BNB 4-bit compute dtype\n",
    "BNB_4BIT_USE_DOUBLE_QUANT=True                      # BNB 4-bit use double quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13c74bb-5e3f-420d-b44d-2a129f7edeb6",
   "metadata": {},
   "source": [
    "## 2.4 LoRA 관련 파라미터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0abd9d3-31a6-4040-94bb-02ba42a673ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "R=8                                                 # Lora attention dimension\n",
    "LORA_ALPHA=16                                       # Lora alpha parameter\n",
    "LORA_DROPOUT=0.05                                   # Lora dropout probability\n",
    "FAN_IN_FAN_OUT=False                                # Lora fan in fan out\n",
    "BIAS=\"none\"                                         # Lora bias type\n",
    "TARGET_MODULES=[\"q_proj\", \"v_proj\"]                 # Lora target modules\n",
    "INFERENCE_MODE=False                                # Inference mode\n",
    "TASK_TYPE=\"CAUSAL_LM\"                               # Task type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de8ac93-8536-4039-b03f-b19d4748d4ad",
   "metadata": {},
   "source": [
    "### 2.5 Tokenizer 관련 파라미터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "abfbd741-c3ae-4e33-88e7-c589d85174a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH=512                                     # Max sequence length for tokenizer\n",
    "TRUNCATION=True                                     # Enable/disable truncation\n",
    "RETURN_OVERFLOWING_TOKENS=True                      # Return overflowing tokens info\n",
    "RETURN_LENGTH=True                                  # Return length of encoded inputs\n",
    "PADDING=True                                        # Enable padding to max sequence length\n",
    "PADDING_SIDE=\"right\"                                # The side on which the model should have padding appliedㅠ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f5f841-5740-49b8-bd57-b1d49ea840f5",
   "metadata": {},
   "source": [
    "### 2.6 PROMPT 관련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "725c36df-4e2d-4cab-ab28-a9a0100a14d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_DICT = {\n",
    "    \"prompt_input\": (\n",
    "        \"아래는 작업을 설명하는 명령어와 추가적 맥락을 제공하는 입력이 짝을 이루는 예제입니다.\\n\"\n",
    "        \"명령어와 입력을 깊게 이해하고 꼼꼼하고 차분하게 응답을 작성하세요.\\n\\n\"\n",
    "        \n",
    "        \"### Instruction(명령어):%s\\n\"\n",
    "        \"### Input(입력):%s\\n\"\n",
    "        \"### Response(응답):\"\n",
    "    ),\n",
    "    \"prompt_no_input\": (\n",
    "        \"아래는 작업을 설명하는 명령어입니다.\\n\"\n",
    "        \"명령어를 깊게 이해하고 꼼꼼하고 차분하게 응답을 작성하세요.\\n\"\n",
    "        \n",
    "        \"### Instruction(명령어):%s\\n\"\n",
    "        \"### Response(응답):\"\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1a71d7-8acd-4bf5-8545-c06ea1256b65",
   "metadata": {},
   "source": [
    "### 2.7 Evaluate 관련"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424f27a7-6f4a-46d6-91eb-5efb00b1a693",
   "metadata": {},
   "source": [
    "* 모델 Generation 결과 데이터 직렬화 관련\n",
    "    * 모델 추론 시 GPU 자원 사용 및 추론 시간이 너무 오래걸려서 결과 데이터를 직렬화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ffb5d412-a299-4db7-8b58-529842c9f171",
   "metadata": {},
   "outputs": [],
   "source": [
    "EVALUATE_DIR = os.path.join(BASE_PATH, 'evaluate', 'pickles')\n",
    "GENERATION_DATA_LIST_PICKLE_NAME = 'sh_polyglot-alpaca_1st_1000_output.pkl'\n",
    "GENERATION_DATA_LIST_PICKLE_PATH = os.path.join(EVALUATE_DIR, GENERATION_DATA_LIST_PICKLE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cfc912e-9677-49d1-84a5-596a83119760",
   "metadata": {},
   "source": [
    "* 모델 평가 결과 파일로 저장 관련 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "92ef66b1-8ed7-4f08-9a76-2432e1c50636",
   "metadata": {},
   "outputs": [],
   "source": [
    "EVALUATION_RESULT_DIR = os.path.join(BASE_PATH, 'evaluate', 'metrics')\n",
    "EVALUATION_RESULT_FILENAME = os.path.join(EVALUATION_RESULT_DIR, 'sh_polyglot-alpaca_1th_evaluation.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336fd965-129f-482c-9d0d-6578bc908852",
   "metadata": {},
   "source": [
    "## 3. 데이터셋 살펴보기 및 하나로 합치기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc2928e-85d0-4ca8-a658-5a657563b573",
   "metadata": {},
   "source": [
    "### 3.1 한국어 Aplaca 데이터셋 살펴보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "28f72900-18b0-40cb-8184-51292fe80874",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = os.path.join(BASE_PATH, 'datas')\n",
    "DATASET_KO_ALPACA_FINAL_PATH = os.path.join(DATA_PATH, 'final_ko_alpaca_data.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3d25aa29-d9ab-4b62-bd25-0b404ea73835",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instruction</th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>이메일에서 스팸을 탐지하기 위한 기계 학습 알고리즘을 만드세요.</td>\n",
       "      <td></td>\n",
       "      <td>이메일 스팸 탐지 알고리즘을 위한 몇 가지 인기있는 기계 학습 모델에는 Suppor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>고기 요리 시 맛술을 사용하는 이유와 대체재는 무엇인가요? 어떤 음식에 맛술을 사용...</td>\n",
       "      <td></td>\n",
       "      <td>안녕하세요! 고기 요리 시 맛술을 사용하는 이유는 냄새와 맛의 개선입니다. 근육 안...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>대화를 시작하기 위해 트윗을 작성합니다.</td>\n",
       "      <td></td>\n",
       "      <td>'안녕하세요! 오늘 날씨가 참 좋네요!'  (트윗 예시)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>저녁과 밤의 구분 기준은 무엇인가요? 저녁 8시가 맞나요? 밤 8시가 맞나요?</td>\n",
       "      <td></td>\n",
       "      <td>저녁과 밤을 구분하는 기준은 명확히 정의되어 있지 않습니다. '저녁'은 해가 지고 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>이 문장의 시작 부분에 전환 단어를 삽입하세요.</td>\n",
       "      <td>그는 오랫동안 수색한 끝에 보물을 찾았습니다.</td>\n",
       "      <td>그러나, 그는 오랫동안 수색한 끝에 보물을 찾았습니다.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         instruction  \\\n",
       "0                이메일에서 스팸을 탐지하기 위한 기계 학습 알고리즘을 만드세요.   \n",
       "1  고기 요리 시 맛술을 사용하는 이유와 대체재는 무엇인가요? 어떤 음식에 맛술을 사용...   \n",
       "2                             대화를 시작하기 위해 트윗을 작성합니다.   \n",
       "3        저녁과 밤의 구분 기준은 무엇인가요? 저녁 8시가 맞나요? 밤 8시가 맞나요?   \n",
       "4                         이 문장의 시작 부분에 전환 단어를 삽입하세요.   \n",
       "\n",
       "                       input  \\\n",
       "0                              \n",
       "1                              \n",
       "2                              \n",
       "3                              \n",
       "4  그는 오랫동안 수색한 끝에 보물을 찾았습니다.   \n",
       "\n",
       "                                              output  \n",
       "0  이메일 스팸 탐지 알고리즘을 위한 몇 가지 인기있는 기계 학습 모델에는 Suppor...  \n",
       "1  안녕하세요! 고기 요리 시 맛술을 사용하는 이유는 냄새와 맛의 개선입니다. 근육 안...  \n",
       "2                    '안녕하세요! 오늘 날씨가 참 좋네요!'  (트윗 예시)  \n",
       "3  저녁과 밤을 구분하는 기준은 명확히 정의되어 있지 않습니다. '저녁'은 해가 지고 ...  \n",
       "4                     그러나, 그는 오랫동안 수색한 끝에 보물을 찾았습니다.  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KO_ALPACA_FINAL_PATH = os.path.join(BASE_PATH, DATASET_KO_ALPACA_FINAL_PATH)\n",
    "\n",
    "with open(KO_ALPACA_FINAL_PATH, 'r', encoding='utf-8') as file:\n",
    "    ko_alpaca_final = json.load(file)\n",
    "\n",
    "df_ko_alpaca_final = pd.DataFrame(ko_alpaca_final)\n",
    "df_ko_alpaca_final.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75964711-34a4-4ef5-94b7-9772eaee5895",
   "metadata": {},
   "source": [
    "### 3.2 평가를 위해 한국어 Aplaca 데이터셋 1000개의 샘플 뽑기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8fc4c263-a25b-44dd-8937-4ef26d4cb079",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instruction</th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17666</th>\n",
       "      <td>고객 서비스 봇에 가장 적합한 어조를 파악하세요.</td>\n",
       "      <td></td>\n",
       "      <td>고객 서비스에서 호칭과 제품 또는 서비스에 대한 지식을 바탕으로 열정적인 어조를 사...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69026</th>\n",
       "      <td>동물 멸종 사건에 대한 뉴스 기사의 헤드라인을 생성합니다.</td>\n",
       "      <td></td>\n",
       "      <td>동물학자들이 말하는 대규모 멸종 사건이 상상이상으로 빨리 벌어지고 있습니다.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20869</th>\n",
       "      <td>예산 수립을 위한 계획 만들기</td>\n",
       "      <td></td>\n",
       "      <td>예산 수립 계획을 만들 때는 다음과 같은 단계를 따릅니다. 예산 오브젝티브 설정, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68447</th>\n",
       "      <td>\"Dear John\"이라는 표현의 어원은 무엇일까요? 왜 이별 편지나 절교장을 의미...</td>\n",
       "      <td></td>\n",
       "      <td>\"Dear John letter\"는 이별 편지나 절교장을 뜻하는 표현으로 사용됩니다...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15376</th>\n",
       "      <td>피보나치 수열의 처음 10개의 숫자를 출력하는 함수를 Java로 작성하십시오.</td>\n",
       "      <td></td>\n",
       "      <td>```java\\npublic class Fibonacci {\\n\\n  public ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7944</th>\n",
       "      <td>지구 온난화로 인해 발생할 수 있는 다섯 가지 문제를 브레인스토밍하십시오.</td>\n",
       "      <td></td>\n",
       "      <td>지구온난화로 인해 발생할 수 있는 다섯 가지 문제는 먹을 것 부족, 극심한 기후변화...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1741</th>\n",
       "      <td>\"별이 태어났다\"라는 문구를 다른 두 가지 방법으로 다시 쓰십시오.</td>\n",
       "      <td></td>\n",
       "      <td>New star is born, A star has arisen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61640</th>\n",
       "      <td>다음 기사의 헤드라인을 작성합니다: \"스타틴이 심장 마비 위험을 줄인다는 연구 결과...</td>\n",
       "      <td></td>\n",
       "      <td>\"연구 결과, 스타틴은 심장 마비 위험을 줄인다.\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43819</th>\n",
       "      <td>해변의 자연물을 사용하여 게임을 디자인하세요.</td>\n",
       "      <td></td>\n",
       "      <td>해변에서 찾은 조개껍질을 이용하여, 조개찾기 대회를 개최해보세요.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56612</th>\n",
       "      <td>스코틀랜드에서 남자가 치마를 입는 이유는 무엇인가요?</td>\n",
       "      <td></td>\n",
       "      <td>스코틀랜드에서 남성이 치마를 입는 이유는 여러 가지 주장이 있으나, 역사적으로는 1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             instruction input  \\\n",
       "17666                        고객 서비스 봇에 가장 적합한 어조를 파악하세요.         \n",
       "69026                   동물 멸종 사건에 대한 뉴스 기사의 헤드라인을 생성합니다.         \n",
       "20869                                   예산 수립을 위한 계획 만들기         \n",
       "68447  \"Dear John\"이라는 표현의 어원은 무엇일까요? 왜 이별 편지나 절교장을 의미...         \n",
       "15376        피보나치 수열의 처음 10개의 숫자를 출력하는 함수를 Java로 작성하십시오.         \n",
       "...                                                  ...   ...   \n",
       "7944           지구 온난화로 인해 발생할 수 있는 다섯 가지 문제를 브레인스토밍하십시오.         \n",
       "1741               \"별이 태어났다\"라는 문구를 다른 두 가지 방법으로 다시 쓰십시오.         \n",
       "61640  다음 기사의 헤드라인을 작성합니다: \"스타틴이 심장 마비 위험을 줄인다는 연구 결과...         \n",
       "43819                          해변의 자연물을 사용하여 게임을 디자인하세요.         \n",
       "56612                      스코틀랜드에서 남자가 치마를 입는 이유는 무엇인가요?         \n",
       "\n",
       "                                                  output  \n",
       "17666  고객 서비스에서 호칭과 제품 또는 서비스에 대한 지식을 바탕으로 열정적인 어조를 사...  \n",
       "69026         동물학자들이 말하는 대규모 멸종 사건이 상상이상으로 빨리 벌어지고 있습니다.  \n",
       "20869  예산 수립 계획을 만들 때는 다음과 같은 단계를 따릅니다. 예산 오브젝티브 설정, ...  \n",
       "68447  \"Dear John letter\"는 이별 편지나 절교장을 뜻하는 표현으로 사용됩니다...  \n",
       "15376  ```java\\npublic class Fibonacci {\\n\\n  public ...  \n",
       "...                                                  ...  \n",
       "7944   지구온난화로 인해 발생할 수 있는 다섯 가지 문제는 먹을 것 부족, 극심한 기후변화...  \n",
       "1741                 New star is born, A star has arisen  \n",
       "61640                       \"연구 결과, 스타틴은 심장 마비 위험을 줄인다.\"  \n",
       "43819               해변에서 찾은 조개껍질을 이용하여, 조개찾기 대회를 개최해보세요.  \n",
       "56612  스코틀랜드에서 남성이 치마를 입는 이유는 여러 가지 주장이 있으나, 역사적으로는 1...  \n",
       "\n",
       "[1000 rows x 3 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ko_alpaca_final_sampled = df_ko_alpaca_final.sample(1000, random_state=RANDOM_SEED)\n",
    "df_ko_alpaca_final_sampled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412d638f-ac2e-4653-b4e3-f497803a1db6",
   "metadata": {},
   "source": [
    "### 3.2 평가를 위해 한국어 Aplaca 데이터셋 1000개의 샘플 뽑기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8f747589-9b35-4212-b630-355b70082bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_prompts = []\n",
    "for _, row in df_ko_alpaca_final_sampled.iterrows():\n",
    "    instruction = row['instruction']\n",
    "    input_text = row['input']\n",
    "    if input_text:\n",
    "        formatted_prompts.append(PROMPT_DICT['prompt_input'] % (instruction, input_text))\n",
    "    else:\n",
    "        formatted_prompts.append(PROMPT_DICT['prompt_no_input'] % instruction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "267aec8f-7339-4e4d-91f3-6591a5c24759",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['아래는 작업을 설명하는 명령어입니다.\\n명령어를 깊게 이해하고 꼼꼼하고 차분하게 응답을 작성하세요.\\n### Instruction(명령어):고객 서비스 봇에 가장 적합한 어조를 파악하세요.\\n### Response(응답):',\n",
       " '아래는 작업을 설명하는 명령어입니다.\\n명령어를 깊게 이해하고 꼼꼼하고 차분하게 응답을 작성하세요.\\n### Instruction(명령어):동물 멸종 사건에 대한 뉴스 기사의 헤드라인을 생성합니다.\\n### Response(응답):']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatted_prompts[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e7c321-9ddd-4130-b2cc-e1e753001a5e",
   "metadata": {},
   "source": [
    "## 4. 모델 호출"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf6bafc-f120-4a63-94d4-4e1e6a560c31",
   "metadata": {},
   "source": [
    "### 4.1 모델 학습을 위한 기본 확인사항 내용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0a29e9-ee01-4da8-ae8f-12f206d26554",
   "metadata": {},
   "source": [
    "* 모델 파라미터 정보 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f2a7d1b4-1ffa-4cb4-bddd-f91bb481462b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    all_params = sum(p.numel() for p in model.parameters())\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_params} || trainable%: {100 * trainable_params / all_params}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5c1fbf-709d-4a37-9888-d0edde974e8a",
   "metadata": {},
   "source": [
    "* GPU 분산학습 설정 사용 유무 점검"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0e46ca01-89fb-4484-bdbc-072ae4e5d475",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddp = False\n",
    "\n",
    "def get_device_map():\n",
    "    print(f\"num_gpus: {torch.cuda.device_count()}\")\n",
    "    world_size = int(os.environ.get(\"WORLD_SIZE\", torch.cuda.device_count()))\n",
    "    print(f\"world_size: {world_size}\")\n",
    "    ddp = world_size != 1\n",
    "    if ddp:\n",
    "        device_map = {\"\": int(os.environ.get(\"LOCAL_RANK\") or 0)}\n",
    "        GRADIENT_ACCUMULATION_STEPS = TRAIN_BATCH_SIZE // world_size\n",
    "        if GRADIENT_ACCUMULATION_STEPS == 0:\n",
    "            GRADIENT_ACCUMULATION_STEPS = 1\n",
    "        print(f\"ddp is on - gradient_accumulation_steps: {GRADIENT_ACCUMULATION_STEPS}\")\n",
    "    else:\n",
    "        device_map = \"auto\"\n",
    "        print(\"ddp is off\")\n",
    "\n",
    "    return device_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3d70bf-e8bc-4533-a6d8-8f39c5540f26",
   "metadata": {},
   "source": [
    "## 4.2 모델 Load"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af73e42-081b-4bca-8569-63768bb5109c",
   "metadata": {},
   "source": [
    "* 양자화 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4847d84f-65a2-4931-8d2e-d78e366115f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=LOAD_IN_4BIT,\n",
    "    bnb_4bit_use_double_quant=BNB_4BIT_USE_DOUBLE_QUANT,\n",
    "    bnb_4xqbit_quant_type=BNB_4BIT_QUANT_TYPE,\n",
    "    bnb_4bit_computxe_dtype=BNB_4BIT_COMPUTE_DTYPE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55f8f3b-4f3a-4a3a-bc18-cb65ea5d7756",
   "metadata": {},
   "source": [
    "* 모델 읽어오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "44d18530-2c5f-4cea-b967-470eaa367a7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_gpus: 1\n",
      "world_size: 1\n",
      "ddp is off\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ee560cb58fb460f90a93397dad1f31f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/686 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9428720c3ff34f889c69065ac881b005",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/52.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e828bedb4edf4c8a88cdb65d8a89781d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbd72169a8e5411c9a223a76b4d9100d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00028.safetensors:   0%|          | 0.00/946M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a56e6bc61764e23a4456c5c49d76818",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00028.safetensors:   0%|          | 0.00/843M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19a657237e314948bb6d668f858b12ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00028.safetensors:   0%|          | 0.00/843M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1553f38efdb4507b2b0b9056a8b7f8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00028.safetensors:   0%|          | 0.00/1.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dd485a19a0d462d8dafb0be843a1fe5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00005-of-00028.safetensors:   0%|          | 0.00/896M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ae936875d424c7da743a885a4856ede",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00006-of-00028.safetensors:   0%|          | 0.00/1.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dcc0ec971e543c79c2c4ae4bc2923db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00007-of-00028.safetensors:   0%|          | 0.00/896M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cff77dfabe474c24bd5e930798272c8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00008-of-00028.safetensors:   0%|          | 0.00/1.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09ab66ea09a4427f962b5dd64b798856",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00009-of-00028.safetensors:   0%|          | 0.00/896M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15eb5f837770426ba028bba6144e922a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00010-of-00028.safetensors:   0%|          | 0.00/1.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5784bff2376c49fc9df4fd235708528e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00011-of-00028.safetensors:   0%|          | 0.00/896M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d285d0c297894105976a2de4c11ac3c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00012-of-00028.safetensors:   0%|          | 0.00/1.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1b07816cbe64141abd3c43396f35193",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00013-of-00028.safetensors:   0%|          | 0.00/896M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5ac64a2d53542508a73c6a87af1c30f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00014-of-00028.safetensors:   0%|          | 0.00/1.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b92f9a9910441ac83247b9a069bdce2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00015-of-00028.safetensors:   0%|          | 0.00/896M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa884aba52dc4ec79e29865d48cbeeaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00016-of-00028.safetensors:   0%|          | 0.00/1.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ceb0b26fc234b42895c3037f99824f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00017-of-00028.safetensors:   0%|          | 0.00/896M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fa5c2b6d9ec41b2a09b049b2640e30d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00018-of-00028.safetensors:   0%|          | 0.00/1.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb5a3ad4fffb469b8e7b55611fe529ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00019-of-00028.safetensors:   0%|          | 0.00/896M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a66b8ed0c350410eabdaa99bfbf31cfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00020-of-00028.safetensors:   0%|          | 0.00/1.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90d4de7b4682465b95d55497a670ef56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00021-of-00028.safetensors:   0%|          | 0.00/896M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "315facb9b9444c1090e33270d948a496",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00022-of-00028.safetensors:   0%|          | 0.00/1.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23c6046b18a94353b4b7470e8b8750b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00023-of-00028.safetensors:   0%|          | 0.00/896M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e3798688bd74e1ab0121a58b74bdb6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00024-of-00028.safetensors:   0%|          | 0.00/1.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a13d2d2fbd3419d85df95a869e997a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00025-of-00028.safetensors:   0%|          | 0.00/896M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1c6c910d9864c7dbd6a895094a112f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00026-of-00028.safetensors:   0%|          | 0.00/1.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b631b7e76bc342c9ab9884ab629231af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00027-of-00028.safetensors:   0%|          | 0.00/896M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0ce923341f441a4b6bc8c2384d9109d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00028-of-00028.safetensors:   0%|          | 0.00/518M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "952b243c71954a5c80695f56f6eb37dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35c66217a95e4dfe818103c4eb640e76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/100 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 308848640 || all params: 6602147840 || trainable%: 4.67800248471867\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME_OR_PATH,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=get_device_map(),\n",
    "    cache_dir=CACHE_DIR,\n",
    "    token=HUGGINGFACE_TOKEN\n",
    ")\n",
    "\n",
    "print_trainable_parameters(model)\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f42641b-7b8d-4f2b-864f-ea1d2215e570",
   "metadata": {},
   "source": [
    "## 4.3 LoRA Adapter 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "370d5dff-bc29-4271-af67-1f5bbc279254",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/peft/tuners/lora/bnb.py:229: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 0 || all params: 6602147840 || trainable%: 0.0\n"
     ]
    }
   ],
   "source": [
    "model = PeftModel.from_pretrained(\n",
    "    model,\n",
    "    CHECKPOINT_DIR,\n",
    "    torch_dtype=torch.float16,\n",
    "    quantization_config=bnb_config,\n",
    ")\n",
    "model = model.merge_and_unload()\n",
    "print_trainable_parameters(model)\n",
    "\n",
    "\n",
    "if not ddp and torch.cuda.device_count() > 1:\n",
    "    model.is_parallelizable = True\n",
    "    model.model_parallel = True\n",
    "    print(\"not ddp - trying its own DataParallelism\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "403cd075-61bf-488a-a6b8-9bc6f6586950",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTNeoXForCausalLM(\n",
       "  (gpt_neox): GPTNeoXModel(\n",
       "    (embed_in): Embedding(30080, 5120)\n",
       "    (emb_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-39): 40 x GPTNeoXLayer(\n",
       "        (input_layernorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_layernorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (attention): GPTNeoXAttention(\n",
       "          (rotary_emb): GPTNeoXRotaryEmbedding()\n",
       "          (query_key_value): Linear4bit(in_features=5120, out_features=15360, bias=True)\n",
       "          (dense): Linear4bit(in_features=5120, out_features=5120, bias=True)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (mlp): GPTNeoXMLP(\n",
       "          (dense_h_to_4h): Linear4bit(in_features=5120, out_features=20480, bias=True)\n",
       "          (dense_4h_to_h): Linear4bit(in_features=20480, out_features=5120, bias=True)\n",
       "          (act): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (embed_out): Linear(in_features=5120, out_features=30080, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eaad891-4961-46fc-8e2b-d8e3d0c52ee2",
   "metadata": {},
   "source": [
    "### 4.3 Tokenizer 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8be6c09-a295-47a0-b348-5ad33bcc1d4b",
   "metadata": {},
   "source": [
    "* 모델 tokenizer를 통해 사용할 데이터셋을 분석하여 입력 데이터의 길이 분포를 파악"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "026bcdcd-dd9f-41c8-9c8b-3c6791bb6310",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4917a2fb5c7c43fab53a6dddf61f5d61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/210 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5234c511e49a4ac4a7d33938e78496cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.65M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dabddee247a14d048d203a12eef96901",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/185 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME_OR_PATH, cache_dir=CACHE_DIR, use_add_token=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f0206a-6472-49a9-877b-f6860c546169",
   "metadata": {},
   "source": [
    "## 5. 모델을 통한 추론"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa4c302-58ce-4286-9945-02b663338f05",
   "metadata": {},
   "source": [
    "### 5.1 추론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "133c0c31-a565-442b-b3e5-9ad5266f6552",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:430: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "streamer = TextStreamer(tokenizer)\n",
    "\n",
    "generation_config = GenerationConfig(\n",
    "    temperature=0.2,\n",
    "    top_p=0.9,\n",
    "    top_k=50,\n",
    "    max_new_tokens=MAX_LENGTH,\n",
    "    early_stopping=True,\n",
    "    do_sample=True,\n",
    "    repetition_penalty=1.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c3423fdd-dd37-4101-b38a-08540620823f",
   "metadata": {},
   "outputs": [],
   "source": [
    "streamer = TextStreamer(tokenizer)\n",
    "\n",
    "def gen(instruction, input=None):\n",
    "    # query = f\"### instruction: {x}\\n\\n### Response: \"\n",
    "    query = PROMPT_DICT['prompt_no_input'] % instruction\n",
    "    if input:\n",
    "        query = PROMPT_DICT['prompt_input'] % (instruction, input)\n",
    "    generated_id = model.generate(\n",
    "        **tokenizer(\n",
    "            query,\n",
    "            return_tensors='pt',\n",
    "            return_token_type_ids=False\n",
    "        ).to('cuda'),\n",
    "        generation_config=generation_config,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        streamer=streamer,\n",
    "    )\n",
    "    generated_text = tokenizer.decode(generated_id[0], skip_special_tokens=True)\n",
    "\n",
    "    response_start_idx = generated_text.find(\"### Response(응답):\") + len(\"### Response(응답):\")\n",
    "    response = generated_text[response_start_idx:].strip()\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8a27e1-bdad-4099-a9f1-e74b02f9615c",
   "metadata": {},
   "source": [
    "## 일반 상식 예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d3aa63a2-5ac8-459c-84db-4ff0999a152c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "아래는 작업을 설명하는 명령어입니다.\n",
      "명령어를 깊게 이해하고 꼼꼼하고 차분하게 응답을 작성하세요.\n",
      "### Instruction(명령어):\n",
      "파이썬 공부를 하고 싶습니다. 파이썬을 처음 공부해봅니다. 어떻게 공부를 시작하면 좋을까요?\n",
      "\n",
      "### Response(응답): 파이썬은 문법이 간단하여 쉽게 배울 수 있고, 다양한 분야에서 활용되기 때문에 많은 사람들이 배우려고 합니다. 따라서, 파이썬을 처음 공부할 때는 쉬운 책이나 인터넷 강의 등을 통해 기초부터 차근차근 배워나가면 됩니다. 또한, 파이썬 커뮤니티 사이트나 유튜브 채널 등에서도 파이썬 학습 자료를 제공하고 있으니 참고하시면 좋겠습니다.<|endoftext|>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'파이썬은 문법이 간단하여 쉽게 배울 수 있고, 다양한 분야에서 활용되기 때문에 많은 사람들이 배우려고 합니다. 따라서, 파이썬을 처음 공부할 때는 쉬운 책이나 인터넷 강의 등을 통해 기초부터 차근차근 배워나가면 됩니다. 또한, 파이썬 커뮤니티 사이트나 유튜브 채널 등에서도 파이썬 학습 자료를 제공하고 있으니 참고하시면 좋겠습니다.'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen(\"\"\"\n",
    "파이썬 공부를 하고 싶습니다. 파이썬을 처음 공부해봅니다. 어떻게 공부를 시작하면 좋을까요?\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2e58f8dc-7686-4d2d-b2e6-5ea8836c0ef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "아래는 작업을 설명하는 명령어와 추가적 맥락을 제공하는 입력이 짝을 이루는 예제입니다.\n",
      "명령어와 입력을 깊게 이해하고 꼼꼼하고 차분하게 응답을 작성하세요.\n",
      "\n",
      "### Instruction(명령어):컴퓨터 공학과의 일반적인 커리큘럼은 어떻게 되나요?\n",
      "### Input(입력):답변을 할 때, 2개의 문장으로 작성해주세요.\n",
      "### Response(응답): 컴퓨터 공학에서는 프로그래밍 언어, 운영체제, 데이터베이스, 네트워크 등 다양한 분야를 배웁니다.<|endoftext|>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'컴퓨터 공학에서는 프로그래밍 언어, 운영체제, 데이터베이스, 네트워크 등 다양한 분야를 배웁니다.'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen(\n",
    "    instruction='컴퓨터 공학과의 일반적인 커리큘럼은 어떻게 되나요?',\n",
    "    input='답변을 할 때, 2개의 문장으로 작성해주세요.'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78c9d21-086c-4aee-b162-ba8b8a24749c",
   "metadata": {},
   "source": [
    "## 단순 코드 계산 예제\n",
    "* 코드에 대한 이해는 llama2 모델에 비해 부족해 보이며 코드에 대한 학습이 추가적으로 필요해 보임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "602871a9-564f-42b4-9251-f78ee674fca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "아래는 작업을 설명하는 명령어입니다.\n",
      "명령어를 깊게 이해하고 꼼꼼하고 차분하게 응답을 작성하세요.\n",
      "### Instruction(명령어):\n",
      "파이썬을 통해 1부터 10까지의 총합을 구하는 코드를 작성해주세요\n",
      "\n",
      "### Response(응답): \n",
      "```python\n",
      "def sum_of_numbers (num1, num2):\n",
      "    for i in range(1, 11):\n",
      "    if num1 == 0:\n",
      "        print(\"종료합니다.\")\n",
      "    elif num2 == 0:\n",
      "        print(\"종료합니다.\")\n",
      "    elif num1 == num2:\n",
      "        print(\"두 수를 더하면 20입니다.\")\n",
      "    return sum_of_numbers\n",
      "```<|endoftext|>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'```python\\ndef sum_of_numbers (num1, num2):\\n    for i in range(1, 11):\\n    if num1 == 0:\\n        print(\"종료합니다.\")\\n    elif num2 == 0:\\n        print(\"종료합니다.\")\\n    elif num1 == num2:\\n        print(\"두 수를 더하면 20입니다.\")\\n    return sum_of_numbers\\n```'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen(\"\"\"\n",
    "파이썬을 통해 1부터 10까지의 총합을 구하는 코드를 작성해주세요\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2aaec2c9-d455-42b7-81b2-6765d1af4508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "아래는 작업을 설명하는 명령어입니다.\n",
      "명령어를 깊게 이해하고 꼼꼼하고 차분하게 응답을 작성하세요.\n",
      "### Instruction(명령어):\n",
      "자바 언어를 통해 1부터 15까지 곱셈하는 코드를 작성해주세요\n",
      "\n",
      "### Response(응답): \n",
      "```java.util.Scanner;\n",
      "float a, b;\n",
      "int calculateFloor(float a, float b);\n",
      "size = a * b;\n",
      "if (size >= 15) {\n",
      "    calculateFloor(size->b, size+=2);\n",
      "} else if (size >= 10) {\n",
      "    calculateFloor(size->b, size+=1);\n",
      "} else if (size >= 7) {\n",
      "    calculateFloor(size->b, size*=7);\n",
      "} else {\n",
      "    calculateFloor(size->b, size/=15);\n",
      "} }\n",
      "}\n",
      "```<|endoftext|>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'```java.util.Scanner;\\nfloat a, b;\\nint calculateFloor(float a, float b);\\nsize = a * b;\\nif (size >= 15) {\\n    calculateFloor(size->b, size+=2);\\n} else if (size >= 10) {\\n    calculateFloor(size->b, size+=1);\\n} else if (size >= 7) {\\n    calculateFloor(size->b, size*=7);\\n} else {\\n    calculateFloor(size->b, size/=15);\\n} }\\n}\\n```'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen(\"\"\"\n",
    "자바 언어를 통해 1부터 15까지 곱셈하는 코드를 작성해주세요\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8390a93d-8969-45d5-93b2-6bb41f8dfacb",
   "metadata": {},
   "source": [
    "## 요약 예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c4a1c6c8-f2e1-450e-9072-e9060cea1b7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "아래는 작업을 설명하는 명령어와 추가적 맥락을 제공하는 입력이 짝을 이루는 예제입니다.\n",
      "명령어와 입력을 깊게 이해하고 꼼꼼하고 차분하게 응답을 작성하세요.\n",
      "\n",
      "### Instruction(명령어):\n",
      "아래와 같은 기사가 있습니다. 기사의 핵심 내용을 추려서 간단하게 요약해주세요.\n",
      "\n",
      "### Input(입력):\n",
      "토트넘(잉글랜드)의 3연승을 이끈 '캡틴' 손흥민(31)이 맨 오브 더 매치(MOM)에 선정됐다. \n",
      "손흥민은 크리스마스 이브인 24일(한국 시각) 영국 런던의 토트넘 홋스퍼 스타디움에서 열린 에버턴과 2023-2024시즌 잉글랜드 프리미어리그(EPL) 18라운드 홈 경기에서 팀의 두 번째 골을 터뜨렸다. \n",
      "토트넘은 손흥민의 득점에 힘입어 2 대 1 승리를 거뒀다. \n",
      "이로써 토트넘은 3연승 행진을 이어갔고, 11승 3무 4패 승점 36을 기록했다. 한 경기를 덜 치른 맨체스터 시티(승점 34)를 제치고 4위로 올라섰다.\n",
      "손흥민은 이날도 왼쪽 측면 공격수로 나섰다. 지난 16라운드 뉴캐슬전(1골 2도움)부터 왼쪽 측면에서 최고 윙어의 면모를 유감없이 발휘하고 있다. \n",
      "17라운드 노팅엄 포레스트전에서는 공격 포인트를 올리지 못했으나, 이날 2경기 만에 다시 득점포를 가동했다.\n",
      "리그 11호 골을 터뜨린 손흥민은 무함마드 살라흐(리버풀), 재러드 보웬(웨스트햄)과 나란히 득점 공동 3위에 올랐다. \n",
      "1위는 14골의 엘링 홀란(맨체스터 시티), 2위는 12골의 도미닉 솔란케(본머스)다.\n",
      "또 손흥민은 리그 반환점을 1경기 남겨둔 시점에서 벌써 지난 시즌 득점 기록을 넘어섰다. \n",
      "스포츠 탈장 부상 여파로 고전했던 지난 시즌에는 10골 6도움을 기록했다. \n",
      "도움 4개를 기록 중인 그는 지난 시즌 공격 포인트 기록 돌파도 눈앞에 두고 있다.\n",
      "EPL 통산 득점 랭킹에서는 아스널의 레전드 이안 라이트(113골)을 넘어섰다. \n",
      "114골로 단독 23위에 오른 손흥민은 120골로 공동 21위인 라힘 스털링(첼시), 스티븐 제라드를 6골 차로 쫓고 있다.\n",
      "손흥민은 경기 후 EPL 사무국이 22947명의 팬을 상대로 진행한 투표에서 67.7%의 압도적인 지지를 받아 MOM에 오르는 영예를 안았다. \n",
      "팀 동료인 굴리엘모 비카리오(15.1%), 페드로 포로(7.8%) 등을 크게 따돌렸다.\n",
      "풀타임을 뛴 손흥민은 1골을 포함해 슈팅 2회, 패스 성공률 71%(24/34), 기회 창출 1회, 볼 터치 56회, 드리블 성공 43%(3/7) 등을 기록했다. \n",
      "축구 통계 매체 '풋몹'은 손흥민에게 팀 내 4번째로 높은 평점 7.8을 부여했다.\n",
      "\n",
      "### Response(응답): 손흥민은 토트넘의 3연승을 이끌며 맨 오브 더 매치에 선정되었으며, EPL에서 득점 공동 3위에 올랐습니다.<|endoftext|>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'손흥민은 토트넘의 3연승을 이끌며 맨 오브 더 매치에 선정되었으며, EPL에서 득점 공동 3위에 올랐습니다.'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen(\"\"\"\n",
    "아래와 같은 기사가 있습니다. 기사의 핵심 내용을 추려서 간단하게 요약해주세요.\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "토트넘(잉글랜드)의 3연승을 이끈 '캡틴' 손흥민(31)이 맨 오브 더 매치(MOM)에 선정됐다. \n",
    "손흥민은 크리스마스 이브인 24일(한국 시각) 영국 런던의 토트넘 홋스퍼 스타디움에서 열린 에버턴과 2023-2024시즌 잉글랜드 프리미어리그(EPL) 18라운드 홈 경기에서 팀의 두 번째 골을 터뜨렸다. \n",
    "토트넘은 손흥민의 득점에 힘입어 2 대 1 승리를 거뒀다. \n",
    "이로써 토트넘은 3연승 행진을 이어갔고, 11승 3무 4패 승점 36을 기록했다. 한 경기를 덜 치른 맨체스터 시티(승점 34)를 제치고 4위로 올라섰다.\n",
    "손흥민은 이날도 왼쪽 측면 공격수로 나섰다. 지난 16라운드 뉴캐슬전(1골 2도움)부터 왼쪽 측면에서 최고 윙어의 면모를 유감없이 발휘하고 있다. \n",
    "17라운드 노팅엄 포레스트전에서는 공격 포인트를 올리지 못했으나, 이날 2경기 만에 다시 득점포를 가동했다.\n",
    "리그 11호 골을 터뜨린 손흥민은 무함마드 살라흐(리버풀), 재러드 보웬(웨스트햄)과 나란히 득점 공동 3위에 올랐다. \n",
    "1위는 14골의 엘링 홀란(맨체스터 시티), 2위는 12골의 도미닉 솔란케(본머스)다.\n",
    "또 손흥민은 리그 반환점을 1경기 남겨둔 시점에서 벌써 지난 시즌 득점 기록을 넘어섰다. \n",
    "스포츠 탈장 부상 여파로 고전했던 지난 시즌에는 10골 6도움을 기록했다. \n",
    "도움 4개를 기록 중인 그는 지난 시즌 공격 포인트 기록 돌파도 눈앞에 두고 있다.\n",
    "EPL 통산 득점 랭킹에서는 아스널의 레전드 이안 라이트(113골)을 넘어섰다. \n",
    "114골로 단독 23위에 오른 손흥민은 120골로 공동 21위인 라힘 스털링(첼시), 스티븐 제라드를 6골 차로 쫓고 있다.\n",
    "손흥민은 경기 후 EPL 사무국이 22947명의 팬을 상대로 진행한 투표에서 67.7%의 압도적인 지지를 받아 MOM에 오르는 영예를 안았다. \n",
    "팀 동료인 굴리엘모 비카리오(15.1%), 페드로 포로(7.8%) 등을 크게 따돌렸다.\n",
    "풀타임을 뛴 손흥민은 1골을 포함해 슈팅 2회, 패스 성공률 71%(24/34), 기회 창출 1회, 볼 터치 56회, 드리블 성공 43%(3/7) 등을 기록했다. \n",
    "축구 통계 매체 '풋몹'은 손흥민에게 팀 내 4번째로 높은 평점 7.8을 부여했다.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e7f05d2c-437a-4d42-8f22-720faeb6abc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "아래는 작업을 설명하는 명령어와 추가적 맥락을 제공하는 입력이 짝을 이루는 예제입니다.\n",
      "명령어와 입력을 깊게 이해하고 꼼꼼하고 차분하게 응답을 작성하세요.\n",
      "\n",
      "### Instruction(명령어):아래와 같은 문장들이 있습니다. 답변을 할때 이 문장들을 2개의 문장으로 요약해주세요. 문장이 2개라는 것은 개행문자가 2개인 것을 의미합니다.\n",
      "### Input(입력):\n",
      "FLAN (Fine-tuned LAnguage Net) 모델은 자연어 처리(NLP) 과제들을 해결하기 위해 'instruction tuning'이라는 기법을 사용하는 언어 모델입니다.\n",
      "이 모델의 핵심 아이디어는 다양한 NLP 과제를 자연어 지시사항 형태로 변형하여 이러한 과제들을 풀도록 fine-tuning하는 것입니다.\n",
      "이를 통해 FLAN 모델은 번역, 상식 추론, 감정 분류 등을 포함한 다양한 NLP 과제를 수행할 수 있도록 fine-tuning 됩니다​.\n",
      "\n",
      "FLAN의 연구 결과에 따르면, 이 모델은 zero-shot 시나리오에서 GPT-3보다 우수한 결과를 보였으며, 많은 task에서는 supervised model과 비슷한 성능을 달성했습니다.\n",
      "특히 자연어 추론(NLI)과 질문응답(QA) 작업에서 효과적이었습니다.\n",
      "Google Research Blog에서는 FLAN이 언어 모델을 사용하여 특정 실제 과제에 대한 지식을 어떻게 풀어내는지에 대해 설명합니다.\n",
      "전통적으로는 레이블이 붙은 데이터셋을 이용해 fine-tuning하는 방법이 많이 사용되었지만, FLAN은 다양한 종류의 지시사항에 대해 모델을 fine-tuning함으로써, 특정 과제가 아닌 일반적인 NLP 과제들을 해결할 수 있게 만듭니다.\n",
      "\n",
      "### Response(응답): FLAN 모델은 자연어 처리(NLP) 과제를 해결하기 위해 'instruction tuning'이라는 기법을 사용하는 언어 모델입니다. FLAN 모델은 다양한 NLP 과제를 해결할 수 있도록 fine-tuning하며, 이를 통해 자연어 추론(NLI) 및 질문응답(QA) 작업에서 높은 성과를 보입니다.<|endoftext|>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"FLAN 모델은 자연어 처리(NLP) 과제를 해결하기 위해 'instruction tuning'이라는 기법을 사용하는 언어 모델입니다. FLAN 모델은 다양한 NLP 과제를 해결할 수 있도록 fine-tuning하며, 이를 통해 자연어 추론(NLI) 및 질문응답(QA) 작업에서 높은 성과를 보입니다.\""
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen(\n",
    "    instruction='아래와 같은 문장들이 있습니다. 답변을 할때 이 문장들을 2개의 문장으로 요약해주세요. 문장이 2개라는 것은 개행문자가 2개인 것을 의미합니다.',\n",
    "    input=\n",
    "\"\"\"\n",
    "FLAN (Fine-tuned LAnguage Net) 모델은 자연어 처리(NLP) 과제들을 해결하기 위해 'instruction tuning'이라는 기법을 사용하는 언어 모델입니다.\n",
    "이 모델의 핵심 아이디어는 다양한 NLP 과제를 자연어 지시사항 형태로 변형하여 이러한 과제들을 풀도록 fine-tuning하는 것입니다.\n",
    "이를 통해 FLAN 모델은 번역, 상식 추론, 감정 분류 등을 포함한 다양한 NLP 과제를 수행할 수 있도록 fine-tuning 됩니다​.\n",
    "\n",
    "FLAN의 연구 결과에 따르면, 이 모델은 zero-shot 시나리오에서 GPT-3보다 우수한 결과를 보였으며, 많은 task에서는 supervised model과 비슷한 성능을 달성했습니다.\n",
    "특히 자연어 추론(NLI)과 질문응답(QA) 작업에서 효과적이었습니다.\n",
    "Google Research Blog에서는 FLAN이 언어 모델을 사용하여 특정 실제 과제에 대한 지식을 어떻게 풀어내는지에 대해 설명합니다.\n",
    "전통적으로는 레이블이 붙은 데이터셋을 이용해 fine-tuning하는 방법이 많이 사용되었지만, FLAN은 다양한 종류의 지시사항에 대해 모델을 fine-tuning함으로써, 특정 과제가 아닌 일반적인 NLP 과제들을 해결할 수 있게 만듭니다.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f0eb7c-30cc-46d1-a59f-ae6275f3e8d8",
   "metadata": {},
   "source": [
    "## One-shot 예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "854d3a66-3488-4a33-9c29-7a0418e94b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "아래는 작업을 설명하는 명령어입니다.\n",
      "명령어를 깊게 이해하고 꼼꼼하고 차분하게 응답을 작성하세요.\n",
      "### Instruction(명령어):\n",
      "Blending is all you need 가 뭐에요?\n",
      "\n",
      "### Response(응답): Blending은 혼합이라는 뜻으로, 이미지의 색상과 톤을 자동으로 조정하여 자연스러운 이미지를 만들어내는 기능입니다. 이 명령어는 이미지 편집 프로그램에서 사용할 수 있습니다.<|endoftext|>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Blending은 혼합이라는 뜻으로, 이미지의 색상과 톤을 자동으로 조정하여 자연스러운 이미지를 만들어내는 기능입니다. 이 명령어는 이미지 편집 프로그램에서 사용할 수 있습니다.'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen(\"\"\"\n",
    "Blending is all you need 가 뭐에요?\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "31832e07-a1c8-4050-8e55-b9adae53b911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "아래는 작업을 설명하는 명령어입니다.\n",
      "명령어를 깊게 이해하고 꼼꼼하고 차분하게 응답을 작성하세요.\n",
      "### Instruction(명령어):\n",
      "질문 : Blending is all you need 가 뭐에요? 이 논문에 대한 abstract의 내용을 하단의 External Generated Knowledge로 드립니다.\n",
      "\n",
      "```\n",
      "대화형 AI 연구에서는 ChatGPT와 같은 모델들을 예로 들며 매개변수가 많은 모델을 개발하는 경향이 눈에 띕니다. \n",
      "이러한 광범위한 모델들은 점점 더 나은 채팅 응답을 생성하는 경향이 있지만, 상당한 계산 자원과 메모리를 요구합니다. \n",
      "이 연구는 중요한 질문을 탐구합니다: \n",
      "작은 모델들의 조합이 협력적으로 단일 큰 모델에 비해 비슷하거나 향상된 성능을 달성할 수 있는가? \n",
      "우리는 \"블렌딩\"이라는 명칭의, 간단하지만 효과적인 여러 대화형 AI 모델들을 통합하는 방법을 소개합니다. \n",
      "실증적 증거는 특정 작은 모델들이 시너지적으로 혼합될 때, 훨씬 더 큰 모델들의 능력을 능가하거나 맞추어 줄 수 있다고 제안합니다. \n",
      "예를 들어, 중간 크기의 모델 세 개(6B/13B 매개변수)만 통합해도 ChatGPT(175B+ 매개변수)와 같은 훨씬 큰 모델의 성능 지표를 능가하거나 맞출 수 있습니다.\n",
      "이 가설은 Chai 연구 플랫폼에서 대규모 사용자 기반을 대상으로 하는 A/B 테스팅 방법론을 사용하여 30일 동안 엄격하게 테스트되었습니다. \n",
      "연구 결과는 컴퓨팅 요구 사항이 급증하지 않는 상황에서 대화형 AI 효율성을 향상시킬 수 있는 \"블렌딩\" 전략의 잠재력을 강조합니다.\n",
      "```\n",
      "\n",
      "### Response(응답): \"Blending is all you need\"는 대화형 AI 모델을 통합하는 방법 중 하나입니다. 작은 모델들을 통합하면 더 큰 모델보다 더 나은 성능을 발휘할 수 있다는 것을 보여줍니다.<|endoftext|>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\"Blending is all you need\"는 대화형 AI 모델을 통합하는 방법 중 하나입니다. 작은 모델들을 통합하면 더 큰 모델보다 더 나은 성능을 발휘할 수 있다는 것을 보여줍니다.'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen(\"\"\"\n",
    "질문 : Blending is all you need 가 뭐에요? 이 논문에 대한 abstract의 내용을 하단의 External Generated Knowledge로 드립니다.\n",
    "\n",
    "```\n",
    "대화형 AI 연구에서는 ChatGPT와 같은 모델들을 예로 들며 매개변수가 많은 모델을 개발하는 경향이 눈에 띕니다. \n",
    "이러한 광범위한 모델들은 점점 더 나은 채팅 응답을 생성하는 경향이 있지만, 상당한 계산 자원과 메모리를 요구합니다. \n",
    "이 연구는 중요한 질문을 탐구합니다: \n",
    "작은 모델들의 조합이 협력적으로 단일 큰 모델에 비해 비슷하거나 향상된 성능을 달성할 수 있는가? \n",
    "우리는 \"블렌딩\"이라는 명칭의, 간단하지만 효과적인 여러 대화형 AI 모델들을 통합하는 방법을 소개합니다. \n",
    "실증적 증거는 특정 작은 모델들이 시너지적으로 혼합될 때, 훨씬 더 큰 모델들의 능력을 능가하거나 맞추어 줄 수 있다고 제안합니다. \n",
    "예를 들어, 중간 크기의 모델 세 개(6B/13B 매개변수)만 통합해도 ChatGPT(175B+ 매개변수)와 같은 훨씬 큰 모델의 성능 지표를 능가하거나 맞출 수 있습니다.\n",
    "이 가설은 Chai 연구 플랫폼에서 대규모 사용자 기반을 대상으로 하는 A/B 테스팅 방법론을 사용하여 30일 동안 엄격하게 테스트되었습니다. \n",
    "연구 결과는 컴퓨팅 요구 사항이 급증하지 않는 상황에서 대화형 AI 효율성을 향상시킬 수 있는 \"블렌딩\" 전략의 잠재력을 강조합니다.\n",
    "```\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ba06ad-7960-4521-8938-790c8f5048cd",
   "metadata": {},
   "source": [
    "## CoT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48beb1e7-1442-43a4-b376-89ec5b4a3bcd",
   "metadata": {},
   "source": [
    "* 수학적 연산을 LLM이 제대로 하지 못함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bdaa638a-dc93-434b-b2c5-b44b2a9bf8cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "아래는 작업을 설명하는 명령어입니다.\n",
      "명령어를 깊게 이해하고 꼼꼼하고 차분하게 응답을 작성하세요.\n",
      "### Instruction(명령어):\n",
      "한 반에 30명의 학생이 있습니다. 그 중 3분의 2가 소녀입니다. 소년은 몇 명입니까?\n",
      "\n",
      "### Response(응답): 13명<|endoftext|>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'13명'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen(\"\"\"\n",
    "한 반에 30명의 학생이 있습니다. 그 중 3분의 2가 소녀입니다. 소년은 몇 명입니까?\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81db0ac-fb10-4456-aa30-2ac666ddc6a1",
   "metadata": {},
   "source": [
    "* CoT의 Few-shot 예제를 추가하였음에도 제대로 답변을 못함\n",
    "    * KoAlpaca 데이터만 학습을 하였기에 LLM이 수학적인 사고를 하지 못함\n",
    "* 다음번에 학습시킬 때에는 CoT 데이터셋을 추가하고 실험을 해서 CoT 문제를 해결해보자!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c174eae5-e28d-43a0-9886-bd414f7cbe21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "아래는 작업을 설명하는 명령어와 추가적 맥락을 제공하는 입력이 짝을 이루는 예제입니다.\n",
      "명령어와 입력을 깊게 이해하고 꼼꼼하고 차분하게 응답을 작성하세요.\n",
      "\n",
      "### Instruction(명령어):한 반에는 소년과 소녀로 이루어져 있으며, 총 60명의 학생이 있습니다. 그 중 3분의 2가 소녀입니다. 소년은 몇 명입니까? input의 예제를 보고 논리적으로 분해하고 생각을 하여 올바른 답변을 해주세요\n",
      "### Input(입력):\n",
      "예제1)\n",
      "먼저, 반 전체 학생 수인 24명 중에서 소녀의 비율을 계산해야 합니다. 소녀는 전체의 4분의 3에 해당합니다.\n",
      "24명을 4로 나누면 각 그룹에 몇 명이 있는지 알 수 있습니다. 24를 4로 나누면 6명입니다.\n",
      "이제 이 수를 3배하면 소녀의 수를 알 수 있습니다. 6명의 3배는 18명입니다. 그러므로 소녀는 18명입니다.\n",
      "전체 학생 수에서 소녀의 수를 빼면 소년의 수를 알 수 있습니다. 24명에서 18명을 빼면 6명이 남습니다.\n",
      "따라서 이 반에는 소년이 6명 있습니다.\n",
      "Response: 6명\n",
      "\n",
      "예제2)\n",
      "먼저, 반 전체 학생 수인 28명 중에서 소녀의 비율을 계산해야 합니다. 소녀는 전체의 3분의 2에 해당합니다.\n",
      "28명을 3으로 나누면 각 그룹에 몇 명이 있는지 알 수 있습니다. 28을 3으로 나누면 9.33, 즉 약 9명입니다.\n",
      "이제 이 수를 2배하면 소녀의 수를 알 수 있습니다. 9명의 2배는 18명입니다. 그러므로 소녀는 18명입니다.\n",
      "전체 학생 수에서 소녀의 수를 빼면 소년의 수를 알 수 있습니다. 28명에서 18명을 빼면 10명이 남습니다.\n",
      "따라서 이 반에는 소년이 10명 있습니다.\n",
      "Response: 10명\n",
      "\n",
      "예제3)\n",
      "먼저, 도서관 전체 책의 수인 30권 중에서 과학 책의 비율을 계산해야 합니다. 과학 책은 전체의 5분의 4에 해당합니다.\n",
      "30권을 5로 나누면 각 그룹에 몇 권이 있는지 알 수 있습니다. 30을 5로 나누면 6권입니다.\n",
      "이제 이 수를 4배하면 과학 책의 수를 알 수 있습니다. 6권의 4배는 24권입니다. 그러므로 과학 책은 24권입니다.\n",
      "전체 책의 수에서 과학 책의 수를 빼면 문학 책의 수를 알 수 있습니다. 30권에서 24권을 빼면 6권이 남습니다.\n",
      "따라서 도서관에는 문학 책이 6권 있습니다.\n",
      "Response : 6권\n",
      "\n",
      "그렇다면 instruction의 정답은??\n",
      "\n",
      "### Response(응답): 소년은 12명, 소녀는 8명입니다.<|endoftext|>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'소년은 12명, 소녀는 8명입니다.'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen(\n",
    "    instruction = '한 반에는 소년과 소녀로 이루어져 있으며, 총 60명의 학생이 있습니다. 그 중 3분의 2가 소녀입니다. 소년은 몇 명입니까? input의 예제를 보고 논리적으로 분해하고 생각을 하여 올바른 답변을 해주세요',\n",
    "    input=\n",
    "\"\"\"\n",
    "예제1)\n",
    "먼저, 반 전체 학생 수인 24명 중에서 소녀의 비율을 계산해야 합니다. 소녀는 전체의 4분의 3에 해당합니다.\n",
    "24명을 4로 나누면 각 그룹에 몇 명이 있는지 알 수 있습니다. 24를 4로 나누면 6명입니다.\n",
    "이제 이 수를 3배하면 소녀의 수를 알 수 있습니다. 6명의 3배는 18명입니다. 그러므로 소녀는 18명입니다.\n",
    "전체 학생 수에서 소녀의 수를 빼면 소년의 수를 알 수 있습니다. 24명에서 18명을 빼면 6명이 남습니다.\n",
    "따라서 이 반에는 소년이 6명 있습니다.\n",
    "Response: 6명\n",
    "\n",
    "예제2)\n",
    "먼저, 반 전체 학생 수인 28명 중에서 소녀의 비율을 계산해야 합니다. 소녀는 전체의 3분의 2에 해당합니다.\n",
    "28명을 3으로 나누면 각 그룹에 몇 명이 있는지 알 수 있습니다. 28을 3으로 나누면 9.33, 즉 약 9명입니다.\n",
    "이제 이 수를 2배하면 소녀의 수를 알 수 있습니다. 9명의 2배는 18명입니다. 그러므로 소녀는 18명입니다.\n",
    "전체 학생 수에서 소녀의 수를 빼면 소년의 수를 알 수 있습니다. 28명에서 18명을 빼면 10명이 남습니다.\n",
    "따라서 이 반에는 소년이 10명 있습니다.\n",
    "Response: 10명\n",
    "\n",
    "예제3)\n",
    "먼저, 도서관 전체 책의 수인 30권 중에서 과학 책의 비율을 계산해야 합니다. 과학 책은 전체의 5분의 4에 해당합니다.\n",
    "30권을 5로 나누면 각 그룹에 몇 권이 있는지 알 수 있습니다. 30을 5로 나누면 6권입니다.\n",
    "이제 이 수를 4배하면 과학 책의 수를 알 수 있습니다. 6권의 4배는 24권입니다. 그러므로 과학 책은 24권입니다.\n",
    "전체 책의 수에서 과학 책의 수를 빼면 문학 책의 수를 알 수 있습니다. 30권에서 24권을 빼면 6권이 남습니다.\n",
    "따라서 도서관에는 문학 책이 6권 있습니다.\n",
    "Response : 6권\n",
    "\n",
    "그렇다면 instruction의 정답은??\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b53cafe-1412-46a2-99ad-83af7fb72078",
   "metadata": {},
   "source": [
    "## 테스트 데이터셋 검증(held-out 데이터는 아님)¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5a6cc925-5454-487e-a0e8-27da03eb9852",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = GenerationConfig(\n",
    "    temperature=0.1,\n",
    "    top_k=1,\n",
    "    num_beams=3,\n",
    "    do_sample=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ae5d1d06-f72e-4149-8587-44d2d7ef2852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "아래는 작업을 설명하는 명령어입니다.\n",
      "명령어를 깊게 이해하고 꼼꼼하고 차분하게 응답을 작성하세요.\n",
      "### Instruction(명령어):고객 서비스 봇에 가장 적합한 어조를 파악하세요.\n",
      "### Response(응답): 친절하고 부드러운 어조가 고객 서비스 봇에 가장 적합합니다.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "아래는 작업을 설명하는 명령어입니다.\n",
      "명령어를 깊게 이해하고 꼼꼼하고 차분하게 응답을 작성하세요.\n",
      "### Instruction(명령어):동물 멸종 사건에 대한 뉴스 기사의 헤드라인을 생성합니다.\n",
      "### Response(응답):\"동물 멸종, 지구상에서 가장 큰 위기 직면\"\n",
      "----------------------------------------------------------------------------------------------------\n",
      "아래는 작업을 설명하는 명령어입니다.\n",
      "명령어를 깊게 이해하고 꼼꼼하고 차분하게 응답을 작성하세요.\n",
      "### Instruction(명령어):예산 수립을 위한 계획 만들기\n",
      "### Response(응답):예산 수립을 위한 계획을 만들기 위해서는 다음과 같은 단계를 따라야 합니다. 1) 목표를 설정하고 2) 예산을 수립하고 3) 예산을 집행합니다.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "아래는 작업을 설명하는 명령어입니다.\n",
      "명령어를 깊게 이해하고 꼼꼼하고 차분하게 응답을 작성하세요.\n",
      "### Instruction(명령어):\"Dear John\"이라는 표현의 어원은 무엇일까요? 왜 이별 편지나 절교장을 의미하는 표현으로 사용되는 것일까요?\n",
      "\n",
      "영어 사전에서 \"Dear\"를 검색하다 보면 여러 가지 뜻이 나오는데, 그 중에 하나가 \"Dear John (letter)\"입니다. 이 용어가 어떤 의미를 가지는 지 자세히 알고 싶습니다.\n",
      "### Response(응답): \"Dear John\"은 이별 편지나 절교장을 의미하는 표현으로 사용됩니다. 이 용어는 \"존에게\"라는 뜻으로, \"존\"은 영어로 \"John\"이라고 쓰입니다. \"Dear John\"은 \"존에게\"라는 의미를 가지고 있습니다. 이 용어는 이별 편지나 절교장을 의미하는 것으로 사용됩니다. 이 용어의 어원은 정확하게 알려진 바는 없지만, \"존\"이라는 이름이 \"John\"으로 쓰이는 것과 관련이 있을 것으로 추측됩니다. 또한, \"Dear John\"은 \"존에게\"라는 의미를 가지고 있기 때문에, \"존에게\"라는 의미를 가진 다른 표현들과도 연관이 있을 것으로 추측됩니다.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "아래는 작업을 설명하는 명령어입니다.\n",
      "명령어를 깊게 이해하고 꼼꼼하고 차분하게 응답을 작성하세요.\n",
      "### Instruction(명령어):피보나치 수열의 처음 10개의 숫자를 출력하는 함수를 Java로 작성하십시오.\n",
      "### Response(응답):int sum(int a, int b) { return a + b * b; }\n",
      "----------------------------------------------------------------------------------------------------\n",
      "아래는 작업을 설명하는 명령어입니다.\n",
      "명령어를 깊게 이해하고 꼼꼼하고 차분하게 응답을 작성하세요.\n",
      "### Instruction(명령어):데이터 보안을 유지할 수 있는 방법을 설명하세요.\n",
      "### Response(응답):비밀번호를 사용하고, 안전한 클라우드 서비스를 사용하며, 데이터를 암호화하고, 보안 소프트웨어를 사용하며, 정기적인 백업을 수행하는 것이 좋습니다.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "아래는 작업을 설명하는 명령어입니다.\n",
      "명령어를 깊게 이해하고 꼼꼼하고 차분하게 응답을 작성하세요.\n",
      "### Instruction(명령어):최소 5개의 규칙을 사용하여 주식 XYZ에 대한 거래 전략을 설계하세요.\n",
      "### Response(응답):1. XYZ 주식의 가격이 상승할 때, 2. XYZ 주식의 가격이 하락할 때, 3. XYZ 주식의 가격이 일정한 범위 내에서 유지될 때, 4. XYZ 주식의 가격이 상승할 때, 5. XYZ 주식의 가격이 하락할 때.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "아래는 작업을 설명하는 명령어입니다.\n",
      "명령어를 깊게 이해하고 꼼꼼하고 차분하게 응답을 작성하세요.\n",
      "### Instruction(명령어):인터넷에서 원격조정이란 게 무엇인가요? 컴퓨터를 멀리서도 원격 조정할 수 있다는데, 그게 가능한 건가요?\n",
      "### Response(응답): 원격조정은 멀리 떨어진 곳에서 컴퓨터를 제어하는 것을 말합니다. 인터넷을 통해 멀리 떨어진 곳에서도 컴퓨터를 제어할 수 있습니다. 원격조정은 주로 네트워크 보안, 원격지원, 원격제어 등의 용도로 사용됩니다. 원격조정은 주로 인터넷을 통해 이루어지기 때문에, 인터넷이 연결되어 있어야 합니다. 원격조정은 주로 컴퓨터를 제어하는 데 사용됩니다. 원격조정은 멀리 떨어진 곳에서도 컴퓨터를 제어할 수 있는 편리한 방법입니다.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "아래는 작업을 설명하는 명령어입니다.\n",
      "명령어를 깊게 이해하고 꼼꼼하고 차분하게 응답을 작성하세요.\n",
      "### Instruction(명령어):숨겨진 비밀 마을을 방문하기로 결정한 캐릭터에 대한 독특한 이야기를 만들어 보세요.\n",
      "### Response(응답):마을에 도착한 후, 마을의 비밀을 파헤치는 모험을 시작하게 됩니다.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "아래는 작업을 설명하는 명령어와 추가적 맥락을 제공하는 입력이 짝을 이루는 예제입니다.\n",
      "명령어와 입력을 깊게 이해하고 꼼꼼하고 차분하게 응답을 작성하세요.\n",
      "\n",
      "### Instruction(명령어):문장을 같은 질문을 하는 수사학적 질문으로 바꾸세요.\n",
      "### Input(입력):사과는 인기 있는 과일입니다.\n",
      "### Response(응답): 사과는 인기 있는 과일인가요?\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['친절하고 부드러운 어조가 고객 서비스 봇에 가장 적합합니다.',\n",
       " '\"동물 멸종, 지구상에서 가장 큰 위기 직면\"',\n",
       " '예산 수립을 위한 계획을 만들기 위해서는 다음과 같은 단계를 따라야 합니다. 1) 목표를 설정하고 2) 예산을 수립하고 3) 예산을 집행합니다.',\n",
       " '\"Dear John\"은 이별 편지나 절교장을 의미하는 표현으로 사용됩니다. 이 용어는 \"존에게\"라는 뜻으로, \"존\"은 영어로 \"John\"이라고 쓰입니다. \"Dear John\"은 \"존에게\"라는 의미를 가지고 있습니다. 이 용어는 이별 편지나 절교장을 의미하는 것으로 사용됩니다. 이 용어의 어원은 정확하게 알려진 바는 없지만, \"존\"이라는 이름이 \"John\"으로 쓰이는 것과 관련이 있을 것으로 추측됩니다. 또한, \"Dear John\"은 \"존에게\"라는 의미를 가지고 있기 때문에, \"존에게\"라는 의미를 가진 다른 표현들과도 연관이 있을 것으로 추측됩니다.',\n",
       " 'int sum(int a, int b) { return a + b * b; }',\n",
       " '비밀번호를 사용하고, 안전한 클라우드 서비스를 사용하며, 데이터를 암호화하고, 보안 소프트웨어를 사용하며, 정기적인 백업을 수행하는 것이 좋습니다.',\n",
       " '1. XYZ 주식의 가격이 상승할 때, 2. XYZ 주식의 가격이 하락할 때, 3. XYZ 주식의 가격이 일정한 범위 내에서 유지될 때, 4. XYZ 주식의 가격이 상승할 때, 5. XYZ 주식의 가격이 하락할 때.',\n",
       " '원격조정은 멀리 떨어진 곳에서 컴퓨터를 제어하는 것을 말합니다. 인터넷을 통해 멀리 떨어진 곳에서도 컴퓨터를 제어할 수 있습니다. 원격조정은 주로 네트워크 보안, 원격지원, 원격제어 등의 용도로 사용됩니다. 원격조정은 주로 인터넷을 통해 이루어지기 때문에, 인터넷이 연결되어 있어야 합니다. 원격조정은 주로 컴퓨터를 제어하는 데 사용됩니다. 원격조정은 멀리 떨어진 곳에서도 컴퓨터를 제어할 수 있는 편리한 방법입니다.',\n",
       " '마을에 도착한 후, 마을의 비밀을 파헤치는 모험을 시작하게 됩니다.',\n",
       " '사과는 인기 있는 과일인가요?']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_texts = []\n",
    "\n",
    "for prompt in formatted_prompts[:10]:\n",
    "    model_input = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True, max_length=MAX_LENGTH).to('cuda:0')\n",
    "    model_input = {key: val.to('cuda:0') for key, val in model_input.items() if key != 'token_type_ids'}\n",
    "\n",
    "    generated_id = model.generate(\n",
    "        **model_input,\n",
    "        max_new_tokens=MAX_LENGTH,\n",
    "        early_stopping=True,\n",
    "        generation_config=generation_config,\n",
    "        output_scores=True,\n",
    "        return_dict_in_generate=False,\n",
    "    )\n",
    "\n",
    "    generated_text = tokenizer.decode(generated_id[0], skip_special_tokens=True)\n",
    "    print(generated_text)\n",
    "    print('-' * 100)\n",
    "    generated_texts.append(generated_text)\n",
    "\n",
    "final_responses = []\n",
    "for output in generated_texts:\n",
    "    response_start_idx = output.find(\"### Response(응답):\") + len(\"### Response(응답):\")\n",
    "    response = output[response_start_idx:].strip()\n",
    "    final_responses.append(response)\n",
    "\n",
    "final_responses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5580deb9-399a-4d3c-b74b-c1d4b3873a02",
   "metadata": {},
   "source": [
    "### 5.2 추론한 내용 다음 번에 재활용하기 위해 pickle 파일로 직렬화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c2b342-b48b-45e6-9398-b8aa57aeac09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(GENERATION_DATA_LIST_PICKLE_PATH, 'wb') as file:\n",
    "    pickle.dump(final_responses, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f087db-15ac-4182-8a6f-5d9fd8481544",
   "metadata": {},
   "source": [
    "### 5.3 저장되어져있는 추론 내용 pickle 파일로 역직렬화를 통한 메모리 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604f2369-f0f4-4059-b527-5471fc6fd21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(GENERATION_DATA_LIST_PICKLE_PATH, 'rb') as file:\n",
    "    generation_data_list = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7683bfe7-240a-4b07-b3e2-fc69aa3da144",
   "metadata": {},
   "source": [
    "## 6. 데이터 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00fe7e2-c6d5-4771-9c09-5882887544fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_list = df_ko_alpaca_final_sampled['output'].tolist()\n",
    "print(len(reference_list))\n",
    "print(reference_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc0ab75-44c4-4ec5-a2be-c6ab2b834e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "import json\n",
    "\n",
    "class LLMEvaluator:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a600b1-8f1e-4b65-8863-dcbcc5813d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = LLMEvaluator()\n",
    "result = evaluator.run_evaluate(\n",
    "    predictions=generation_data_list, \n",
    "    origins=reference_list,\n",
    "    use_save_as_file=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3820d35c-9f28-4b48-9854-572d043cd473",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c296153-cb76-4ea3-8ae6-410a60ff56c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
